# ğŸ§  **Retrieval-Augmented Classification Framework for Medical Abstracts**

This project presents a **complete end-to-end NLP case study** that builds a **hybrid medical text classification system**.
Instead of depending on a single model, the framework combines:

* **Fine-tuned Transformer (DeBERTa-v3-xsmall)**
* **Retrieval-Augmented Generation (RAG) using Sentence-BERT + FAISS + FLAN-T5-XL**

The objective is to evaluate *which method performs better for short clinical text classification* and how retrieval can enhance interpretability.

The project includes:
**Preprocessing â†’ Embedding â†’ FAISS Indexing â†’ Retrieval â†’ Transformer Fine-Tuning â†’ RAG Classification â†’ Evaluation â†’ Visualization â†’ Comparison**


## ğŸ“Œ **Architecture Diagram**

<img width="1498" height="840" alt="Architecture Diagram" src="https://github.com/user-attachments/assets/b4ea26bb-10e4-4613-bcd8-f4c7cec966dd" />


## ğŸ“˜ **About the Project**

Medical abstracts are classified into **four disease categories**:

* **Neoplasms**
* **Digestive System Diseases**
* **Nervous System Diseases**
* **Cardiovascular Diseases**

To achieve this, the project implements two independent NLP paths:


### ğŸ”µ **1. Transformer-Based Classification (DeBERTa-v3-xsmall)**

A fine-tuned **Transformer model** trained on tokenized and cleaned medical abstracts.

**Why this path?**

* Strong baseline accuracy
* Consistent performance
* Efficient for short-text classification


### ğŸŸ  **2. Retrieval-Augmented Classification (RAG)**

This hybrid pipeline uses:

* **Sentence-BERT** â†’ To generate dense semantic embeddings
* **FAISS** â†’ For fast similarity search
* **FLAN-T5-XL** â†’ To classify using retrieved contextual examples

**Why this path?**

* Adds interpretability
* Retrieves similar abstracts
* Offers context-aware reasoning


### ğŸ§ª **What This Study Compares**

* **Accuracy differences** between Transformer vs. RAG
* **Influence of retrieval** on decision-making
* **Error patterns** across the two pipelines
* **Explainability of predictions** through retrieved examples

This demonstrates how **hybrid NLP systems** can be used to improve decision transparency in medical text analysis.


## ğŸ¯ **Project Outcomes**

### âœ”ï¸ **DeBERTa Transformer Performance**

* **Accuracy:** **ğŸŸ© 82.51%**
* *Stable, consistent performance*
* *Does not require external context*


### âœ”ï¸ **RAG Pipeline Performance**

* **Accuracy:** **ğŸŸ§ 73.20%**
* *More interpretable due to retrieved abstracts*
* *Suitable for explanation-based tasks*


### âœ”ï¸ **Key Insights**

* **Transformers outperform RAG** for pure accuracy.
* **RAG enhances interpretability**, retrieval shows *why* a class was chosen.
* A **combined approach is beneficial** for explainable AI and healthcare NLP.
* Demonstrates a *realistic, modern NLP pipeline* used in industry.


## ğŸš€ **How to Run the Project**

### **1ï¸âƒ£ Clone the Repository**

```bash
git clone https://github.com/your-username/Text-Analytics-Case-Study.git
cd Text-Analytics-Case-Study
```


### **2ï¸âƒ£ Launch the Notebook**

```bash
jupyter notebook Case_study.ipynb
```


### **3ï¸âƒ£ Add Your Dataset**

Place your dataset inside:

```
data/
```


### **4ï¸âƒ£ Run All Notebook Cells**

The pipeline includes:

* **Preprocessing & EDA**
* **Sentence-BERT Embeddings**
* **FAISS Index Creation**
* **Transformer Fine-Tuning**
* **RAG Classification**
* **Visualization & Comparison**

Each step is clearly structured inside the notebook.


## ğŸ™Œ **Acknowledgements**

A special thanks to the open-source community and tools that made this project possible:

* **HuggingFace Transformers** â€“ DeBERTa & FLAN-T5
* **BAAI bge-small** â€“ High-quality embedding model
* **FAISS (Facebook AI)** â€“ Fast similarity search
* **Google FLAN-T5-XL** â€“ Context-aware text generation
* **PyTorch** â€“ Deep learning framework powering the models


